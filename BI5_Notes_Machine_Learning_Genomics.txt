================================================================================
PRACTICAL 5: MACHINE LEARNING FOR GENOMIC DATA CLASSIFICATION
================================================================================

OVERVIEW:
---------
This practical applies machine learning algorithms (Random Forest and SVM) to 
classify genomic sequences based on k-mer features. It demonstrates how ML can 
be used to identify patterns in biological sequences for disease classification.

KEY CONCEPTS:
-------------
1. K-mers: Subsequences of length k from a sequence
   - Example: 3-mers (trigrams) from "ATGC" = ["ATG", "TGC"]
   - Used as features for machine learning
   - Capture local sequence patterns

2. Feature Extraction: Converting sequences to numerical features
   - K-mers are counted to create feature vectors
   - Each k-mer becomes a feature
   - Creates high-dimensional sparse vectors

3. Random Forest: Ensemble learning method
   - Multiple decision trees vote on classification
   - Handles high-dimensional data well
   - Provides feature importance scores
   - Less prone to overfitting than single trees

4. SVM (Support Vector Machine): Classification algorithm
   - Finds optimal hyperplane to separate classes
   - Linear kernel for high-dimensional data
   - Good for binary and multi-class classification
   - Effective with many features

5. Classification Metrics:
   - Accuracy: Overall correctness
   - Precision: True positives / (True positives + False positives)
   - Recall: True positives / (True positives + False negatives)
   - F1 Score: Harmonic mean of precision and recall
   - Confusion Matrix: Shows classification errors

METHODOLOGY:
------------
1. Load genomic sequences and class labels
2. Extract k-mers (3-mers) from each sequence
3. Convert k-mers to feature vectors using CountVectorizer
4. Split data into training and testing sets
5. Train Random Forest and SVM classifiers
6. Evaluate performance using multiple metrics
7. Visualize confusion matrices
8. Analyze feature importance (top k-mers)

LIBRARIES USED:
---------------
- sklearn: Machine learning algorithms and metrics
- pandas: Data handling
- numpy: Numerical operations
- matplotlib/seaborn: Visualization

================================================================================
VIVA QUESTIONS & ANSWERS:
================================================================================

Q1: What are k-mers and why are they used?
A1: K-mers are:
    - Subsequences of length k from a sequence
    - Example: 3-mers from "ATGC" = ["ATG", "TGC"]
    - Capture local sequence patterns
    - Used as features for machine learning
    - Simple but effective for sequence classification

Q2: Why do we use k-mers instead of full sequences?
A2: K-mers are used because:
    - Sequences have variable lengths
    - K-mers create fixed-length feature vectors
    - Capture local patterns important for classification
    - More interpretable than raw sequences
    - Work well with standard ML algorithms

Q3: What is Random Forest?
A3: Random Forest is:
    - Ensemble of decision trees
    - Each tree votes on classification
    - Final prediction is majority vote
    - Handles high-dimensional data well
    - Provides feature importance
    - Less prone to overfitting

Q4: What is SVM (Support Vector Machine)?
A4: SVM is:
    - Classification algorithm that finds optimal separating hyperplane
    - Maximizes margin between classes
    - Works well with high-dimensional data
    - Linear kernel often used for text/sequence data
    - Effective for both binary and multi-class problems

Q5: What is feature extraction in the context of sequences?
A5: Feature extraction:
    - Converts sequences to numerical features
    - K-mers are counted to create feature vectors
    - Each unique k-mer becomes a feature dimension
    - Creates sparse high-dimensional vectors
    - Enables use of standard ML algorithms

Q6: What is the difference between precision and recall?
A6: Precision:
    - Of predicted positives, how many are actually positive?
    - Precision = TP / (TP + FP)
    - Measures accuracy of positive predictions
    Recall:
    - Of actual positives, how many did we find?
    - Recall = TP / (TP + FN)
    - Measures completeness of positive detection

Q7: What is F1 score?
A7: F1 score is:
    - Harmonic mean of precision and recall
    - F1 = 2 × (Precision × Recall) / (Precision + Recall)
    - Balances precision and recall
    - Useful when classes are imbalanced
    - Single metric summarizing performance

Q8: What is a confusion matrix?
A8: Confusion matrix:
    - Table showing classification results
    - Rows = actual classes, Columns = predicted classes
    - Shows true positives, false positives, true negatives, false negatives
    - Helps identify which classes are confused
    - Visual representation of classification errors

Q9: Why do we split data into training and testing sets?
A9: Data splitting:
    - Training set: Used to learn model parameters
    - Testing set: Used to evaluate generalization
    - Prevents overfitting (memorizing training data)
    - Provides unbiased performance estimate
    - Standard practice: 80% train, 20% test

Q10: What is feature importance in Random Forest?
A10: Feature importance:
     - Measures how much each feature contributes to classification
     - Based on how often feature is used in splits
     - Higher importance = more predictive power
     - Helps identify important k-mers
     - Useful for feature selection

Q11: How do you choose the k value for k-mers?
A11: K value selection:
     - k=3 (trigrams): Good balance, captures local patterns
     - k=4-5: More specific, fewer features
     - k=2: Too short, less informative
     - k>5: Too specific, sparse features
     - Often k=3-4 works well for sequences

Q12: What is the difference between Random Forest and SVM?
A12: Random Forest:
     - Ensemble method (multiple trees)
     - Non-linear decision boundaries
     - Provides feature importance
     - Handles missing values
     SVM:
     - Single model with optimal hyperplane
     - Linear or non-linear (via kernels)
     - Good with high-dimensional data
     - May need feature scaling

Q13: What is overfitting and how do we prevent it?
A13: Overfitting:
     - Model memorizes training data
     - Performs well on training but poorly on test data
     - Prevention methods:
       - Train/test split
       - Cross-validation
       - Regularization
       - Ensemble methods (Random Forest)
       - Feature selection

Q14: How can machine learning help in genomics?
A14: ML in genomics helps:
     - Classify sequences (disease vs healthy)
     - Predict protein function
     - Identify regulatory elements
     - Discover biomarkers
     - Personalize medicine
     - Analyze large-scale genomic data

Q15: What are the limitations of using k-mers for classification?
A15: Limitations:
     - Ignores long-range dependencies
     - High-dimensional sparse features
     - May miss structural patterns
     - Context-independent (order matters but not captured well)
     - Better methods exist (deep learning, transformers)

================================================================================
COMMON ERRORS & TROUBLESHOOTING:
================================================================================
1. Memory errors: Large k-mer feature matrices can be huge
2. Imbalanced classes: May need class weighting or resampling
3. Overfitting: Use cross-validation and regularization
4. Slow training: Consider feature selection or dimensionality reduction
5. Poor performance: Try different k values or algorithms

================================================================================
EXTENSIONS & ADVANCED TOPICS:
================================================================================
- Deep learning (CNNs, RNNs, Transformers) for sequences
- Cross-validation for robust evaluation
- Feature selection techniques
- Hyperparameter tuning
- Ensemble methods
- Handling imbalanced datasets
- Sequence alignment-based features
- Phylogenetic features

================================================================================

